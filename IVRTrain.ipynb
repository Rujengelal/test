{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7104049,"sourceType":"datasetVersion","datasetId":4095358}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q bitsandbytes datasets accelerate loralib\n!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git\n!pip install -q scipy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-02T08:47:26.383174Z","iopub.execute_input":"2023-12-02T08:47:26.383444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.is_available()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport bitsandbytes as bnb\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    \"NousResearch/Llama-2-7b-chat-hf\",\n    torch_dtype=torch.float16,\n    load_in_8bit=True,\n    device_map='auto',\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in model.parameters():\n  param.requires_grad = False\n  if param.ndim == 1:\n    param.data = param.data.to(torch.float32)\n\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\nclass CastOutputToFloat(nn.Sequential):\n  def forward(self, x): return super().forward(x).to(torch.float32)\nmodel.lm_head = CastOutputToFloat(model.lm_head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    # target_modules=[\"query_key_value\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, Dataset\ndf = pd.read_csv(\"/kaggle/input/ivr-hedis/IVR_Questions.csv\")\ndataset = Dataset.from_pandas(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\ndef generate_prompt(hedis_measure: str, question:str) -> str:\n  prompt = f\"### INSTRUCTION\\nBelow is the Hedis Measure and IVR survey questions for a customer. Please write an IVR message for informing customer about their hedis measure.\\n\\n### Hedis Measure:\\n{hedis_measure}\\n### SMS:\\n{question}\"\n  return prompt\n\nmapped_dataset = dataset.map(lambda samples: tokenizer(generate_prompt(samples['Hedis Measures'], samples['IVR'])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = transformers.Trainer(\n    model=model,\n    train_dataset=mapped_dataset,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=20,\n        gradient_accumulation_steps=4,\n        warmup_steps=100,\n        max_steps=-1,\n        num_train_epochs=10,\n        learning_rate=1e-3,\n        fp16=True,\n        logging_steps=1,\n        output_dir='outputs',\n        report_to='tensorboard'\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n)\nmodel.config.use_cache = False\nwith torch.autocast(\"cuda\"):\n    trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained('./ivr_model_llma_final')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"ivr_model_llma_final\"\n\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\npeft_model_id = \"ivr_model_llma_final/\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nmodel = PeftModel.from_pretrained(model, peft_model_id)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T09:05:49.687300Z","iopub.execute_input":"2023-12-02T09:05:49.687561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, Markdown\n\ndef make_inference(hedis_measure):\n\n    batch = tokenizer(f\"### Below is the Hedis Measure of a customer. Please generate three questions for the customer.\\n\\n### Hedis Measure:\\n{hedis_measure}\\n\", return_tensors='pt')\n    batch = batch.to(torch.device('cuda'))\n\n    with torch.cuda.amp.autocast():\n      output_tokens = model.generate(**batch, max_new_tokens=250)\n    # print(tokenizer.decode(output_tokens[0]))\n    display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n# hedis_measure = \"Cardiac Rehabiliation\"\nhedis_measure = \"Controlling High Blood Pressure\"\nmake_inference(hedis_measure)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}